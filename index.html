<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
	<link rel='shortcut icon' href='img/favicon/fav1.gif' type='image/x-icon'/ >
    <title>Nivedha Balakrishnan</title>
	
    <link href="css/bootstrap/css/bootstrap.min.css" rel="stylesheet">
	<link href="css/styles.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="css/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="css/fonts/fonts.css" rel="stylesheet" type="text/css">
</head>

<body id="page-top" class="index">
    <nav id="mainNav" class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">Nivedha Balakrishnan</a>
            </div>
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                   <!-- <li class="hidden">
                        <a href="#page-top"></a>
                    </li>-->
					<li>
                        <a class="page-scroll" href="#page-top">Home</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#projects">Projects</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#experience">Experience</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <header>
        <div class="container">
            <div class="intro-text">
				<!-- <div class="profile-pic">
					<img class="img-circle img-responsive" src="img/pic/profile.JPG" alt="Hello" width="150" height="150">  -->
				<!-- </div>  -->
                <!-- <div class="intro-lead-in">Welcome To My Website!</div>
                <div class="intro-heading">Glad to see you here</div> -->
                <div class="intro-explore"><i>My Portfolio</i></div>
                <!-- <a href="files/Nivedha_Balakrishnan.pdf" target="_blank" class="page-scroll btn btn-xl">Resume</a> -->
            </div>  
        </div>
    </header>
	
    <section id="about" class="bg-light-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-9 text-center">
                    <div>
                        <h2 class="section-heading">About</h2>
                        <p class="large" style="text-align: justify;">
                            I am a passionate data enthusiast with a strong interest in Machine Learning, data science and analytics. I enjoy applying data science techniques to solve real-world problems and have a keen eye for creating easy-to-understand visualizations to communicate complex information. Also, I have expertise in building predictive models using machine learning algorithms to uncover hidden insights in the data. Currently, I am working as a Graduate Research Assistant, where I apply machine learning to identify potential drug targets.    
                        </p>
                    </div>
                    <div class="accomplishments">
                        <h4>Accomplishments</h4>
                        <p>
                            <ul>
                                <li>
                                    Received <strong>Patent from Intellectual Property India</strong> for an innovative product <i>Nylon Fabricated Bone Immobilizer using Rapid Prototyping</i> in the field of orthopedics. 
                                </li>
                                <li>
                                    Received Best Outgoing Student of the Year 2016 from Sri Ramakrishna Engineering College during my Bachelors.
                                </li>
                            </ul>
                        </p>
                    </div>
                </div>
				<!-- <div class="col-lg-4 text-center"> -->
				<!-- <div class="LI-profile-badge"  data-version="v1" data-size="medium" data-locale="en_US" data-type="horizontal" data-theme="dark" data-vanity="sindhubkr"><a class="LI-simple-link" href='https://www.linkedin.com/in/sindhubkr?trk=profile-badge'>Sindhu Balakrishnan</a></div> -->
                
                <div class="pic-wrapper col-lg-3 text-center">
                    <img src="img/pic/niv.jpg" class="profile-image">
                        <ul class="list-inline social-buttons">
                            <li><a href="https://www.linkedin.com/in/nivedhabkr" target="_blank"><i class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a href="https://github.com/NivedhaBalakrishnan" target="_blank"><i class="fa fa-github"></i></a>
                            </li>
                        </ul>
                        <div class="resume-button">
                        <a href="files/Nivedha_Balakrishnan.pdf" target="_blank" class="page-scroll btn btn-xl">Resume</a>
                        </div>
                </div>
                <!-- <a href="files/Sindhu_Balakrishnan.pdf" target="_blank" class="page-scroll btn btn-xl">Resume</a> -->
            </div>
           
            
    </section>

   
    <section id="projects" class="bg-teal-mid">
        <div>
            <div class="col-lg-12 text-center" style="margin-bottom: 30px;">
                <h2 class="section-heading" style="margin-bottom: 0px;">Projects</h2>
                <div style="font-size: 16px"><i>(Click each tile to learn more)</i></div>
            </div>
        </div>


        <main class="gallery">
            <ul class="gallery__list grid">
              <li data-id="project_1" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/drug1.jpg">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                            <h4>Discovering the Undiscovered</h4> 
                            <p>Using ML models to find new drugs</p> 
                        </div>
                      </div>
                    </div>
                  </div>
              </li>

              <li data-id="project_2" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/age1.jpeg" alt="Aging Clock">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>AGING CLOCK</h4> 
                        <br>
                        <p>Prediction of Age using Biomarkers</p> 
                        </div>
                      </div>
                    </div>
                  </div>
              </li>

              <li data-id="project_3" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/dating.jpg" alt="dating-app">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Swipe Right</h4> 
                        <p>A Comparative Analysis of Popular Dating Apps</p> 
                        </div>
                      </div>
                    </div>
                  </div>
              </li>

              <li data-id="project_4" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/car3.jpeg" alt="Small Prominences">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Driven by Data</h4> 
                        <p>Big Data Analytics of the Preowned Car Market</p> 
                        </div>
                      </div>
                    </div>
                  </div>
              </li>
              <li data-id="project_5" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/covid.jpeg" alt="Small Prominences">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Mapping the Pandemic</h4> 
                        <p>A Geographic Analysis of COVID Data in US</p> 
                        </div>
                      </div>
                    </div>
                  </div>
              </li>
              <li data-id="project_6" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/nba1.jpg" alt="Polar Ice">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Dribbling Data</h4>  
                        <p>A Database Management Project Analyzing NBA Stats</p> 
                        </div>
                      </div>
                    </div>
                  </div>
              </li>
              <li data-id="project_7" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/behavior.jpeg" alt="Jupiter">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Coming Soon</h4> 
                        <!-- <p>Using Machine Learning to find new drugs</p>  -->
                        </div>
                      </div>
                    </div>
                  </div>
              </li>
              <li data-id="project_8" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/recommender.webp" alt="Recommender System">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Coming Soon</h4> 
                        <!-- <p>Using Machine Learning to find new drugs</p>  -->
                        </div>
                      </div>
                    </div>
                  </div>
              </li>
              <li data-id="project_9" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/segmentation.jpeg" alt="Image Segmentation">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Coming Soon</h4> 
                        <!-- <p>Using Machine Learning to find new drugs</p>  -->
                        </div>
                      </div>
                    </div>
                  </div>
              </li>
              <li data-id="project_10" class="item">
                <div class="flip-card">
                    <div class="flip-card-inner">
                      <div class="flip-card-front">
                        <figure>
                            <img src="img/chatbot.jpg" alt="Chatbot">
                          </figure>
                      </div>
                      <div class="flip-card-back">
                        <div class="center-text">
                        <h4>Coming Soon</h4> 
                        <!-- <p>Using ML models to find new drugs</p>  -->
                        </div>
                      </div>
                    </div>
                  </div>
              </li>
            </ul>
          </main>
    </section>
	
    <section id="experience" class="bg-light-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Education</h2>
                </div>
            </div>
            <div class="row text-center">
                <div class="col-md-4">
                    <span class="fa-stack fa-4x">
						<img class="img-circle img-responsive" src="img/logos/graduate.png" alt="">
                    </span>
                    <h4 class="service-heading">San Jose State University<br> MS Data Analytics</h4>
                    <h4 class="service-heading"></h4>
                </div>
                <div class="col-md-4">
                    <span class="fa-stack fa-4x">
                        <img class="img-circle img-responsive" src="img/logos/graduate.png" alt="">
                    </span>
                    <h4 class="service-heading">Linkoping University<br> MS Biomedical Engineering</h4>
                    <h4 class="service-heading"></h4>
                </div>
                <div class="col-md-4">
                    <span class="fa-stack fa-4x">
                        <img class="img-circle img-responsive" src="img/logos/graduate.png" alt="">
                    </span>
                    <h4 class="service-heading">Anna University<br> BE Biomedical Engineering</h4>
                    <h4 class="service-heading"></h4>
                </div>
            </div>
        </div>
       
        <div class="container" style="margin-top:50px">
            <div class="row" style="margin-bottom:20px">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading" style="margin-top:30px">Work Experience</h2>
                    <!-- <h3 class="section-subheading text-muted"></h3> -->
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12">
                    <ul class="timeline">
						<li class="timeline-inverted">
                            <div class="timeline-image">
                                <img class="img-responsive" src="img/logos/prog1.png" alt="">
                            </div>
                            <div class="timeline-panel">
                                <div class="timeline-heading">
                                    <h4>June 2016 - July 2017</h4>
                                    <h4 class="subheading">Cognizant Technologies Solutions, India <br> Computer Programmer
									<!-- <a href="http://www.appviewx.com/" target="_blank">AppViewX</a></h4> -->
                                </div>
                            </div>
                        </li>
                        <li>
                            <div class="timeline-image">
                                <img class="img-responsive" src="img/logos/intern1.png" alt="">
                            </div>
                            <div class="timeline-panel">
								<div class="timeline-heading">
                                    <h4>May 2019 - Jan 2020</h4>
                                    <h4 class="subheading">Integrum ab, Sweden <br> Graduate Intern</h4>
                                </div>
                            </div>
                        </li>
                        <li class="timeline-inverted">
                            <div class="timeline-image">
                                <img class="img-responsive" src="img/logos/lab1.png" alt="">
                            </div>
                            <div class="timeline-panel">
								<div class="timeline-heading">
                                    <h4>Jan 2020 - Present</h4>
                                    <h4 class="subheading">San Jose State University <br> Graduate Research Assistant</h4>
                                </div>
                            </div>
                        </li>
                        <li class="timeline-inverted">
                            <div class="timeline-image">
                                <h4>NEXT
                                    <br>BIG
                                    <br>THING!</h4>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

	 <section id="contact" class="bg-darkest-gray">
		<div class="container">
			<div class="row">
				<div class="col-lg-12 text-center">
					<h2 class="section-heading">Message Me</h2>
					<p class="contact-text">
						<p class="text-muted">Feel free to message me for any career opportunities pertaining to Data Analytics or Data Science, you can also drop an email to nivedha.balakrishnan@sjsu.edu</p>
						<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSfYdBIxoq2-7gC5nYJA_Bca5M3rEdCK5vFpFXtshyuO4lK_eA/viewform?usp=pp_url" width="100%" height="450" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>
					</p>
				</div>
			</div>
		</div>
	 </section>

     <div class="overlay-container bg-light-gray">
        <div class="overlay-content">
            <div class="overlay-close-icon">
                <img class="img-responsive" src="img/logos/x-lg.svg" alt="">
            </div>

            <!-- Project details here -->
            <!-- Drug Discovery -->
            <div class="project-details" id="project_1">
                <h1><font size="10">Discover the Undiscovered</font>
                    <br>Using ML models to find new drugs</br>
                </h1>
                
            <div>
                <h2>Problem Statement</h2>
                <p align="justify">
                    Our project focuses on using machine learning models for drug discovery in the field of thrombosis, a leading cause of morbidity and mortality. Thrombin is a key enzyme involved in clot formation, and current direct thrombin inhibitors (DTI) have limitations due to their association with bleeding or thrombotic risks in certain situations. Therefore, discovering new antithrombotic agents with improved safety profiles is essential. By leveraging machine learning models, we aim to identify potential compounds with antithrombotic activity and prioritize them for further experimental validation. Our approach can accelerate the drug discovery process and lead to the development of effective and safe antithrombotic agents.
                </p>
            </div>
        
                <h2>Goal</h2>
                <p align="justify">
                    The goal of our project is to develop a two-staged machine learning model that can predict new antithrombotic peptides. Using machine learning models, we can efficiently screen a vast chemical and biological space and accurately predict new molecules in silico. The first stage of our model will use classification models to identify peptides with thrombin inhibitory activity. The second stage will use regression models to rank peptides based on their effective thrombin inhibitory potential. By successfully developing this two-staged machine learning model, we aim to accelerate the discovery of novel and effective antithrombotic peptides.
                </p>
        
                <h2>Data Collection and Feature Extraction</h2>
                <p align="justify">
                    We curated a positive antithrombotic peptide dataset from peer-reviewed publications, which includes 109 peptides out of which 75 has inhibitory constant (Ki) value. Additionally, we collected a negative peptide dataset from NCBI and Uniprot protein databases, comprising 1065 peptides. These datasets will be used to train and validate our machine learning models for predicting new antithrombotic peptides.
                </p>
                <p align="justify">
                    We then extracted features from these peptides using the BioPython module, a widely used open-source tool for computational biology. These extracted features will be used as inputs to train and validate our machine learning models for predicting new antithrombotic peptides. The figure below shows the extraction features along with its counts. Thus, in total we have 572 features for each peptides.

                </p>
                <center><img src="drug/features.png" height="50%" width="50%"></center>    
                
                
                <h2>Exploratory Data Analysis</h2>
                <p alingn="justify">
                    Exploratory Data Analysis (EDA) is a crucial step in any data analysis project as it helps in understanding the differences in characteristics and properties of the positive and negative peptides. In this project, the data is analyzed through visualizations in using Matplotlib and Seaborn libraries.
                </p>
                    <h3>Analysis of Features</h3>
                    <div class="clearfix">
                        <div class="img-container">
                            <img src="drug/Slide2.PNG" alt="Italy" style="width:100%">
                        </div>
                        <div class="img-container">
                        <img src="drug/Slide3.PNG" alt="Forest" style="width:100%">
                        </div>
                    </div>
                    <h4>Observation</h4>
                    <p>
                        <ul>
                            <li>The positive antithrombotic peptide dataset we curated consists of more peptides with a sequence length of 3-40, while the number of peptides with an increase in sequence length is comparatively lower.</li>
                            <li>Our analysis indicates that the amino acids R, E, P, and F are present at a higher frequency in the positive antithrombotic peptide dataset. This suggests that these amino acids may play a crucial role in the thrombin inhibitory activity of these peptides.</li>  
                            <li>Also, we observed that the positive antithrombotic peptide dataset has a relatively higher proportion of peptides with a negative net charge and a greater helical content compared to the negative peptide dataset. These findings suggest that the presence of a negative charge and helical structure may potentially contribute to the thrombin inhibitory activity of these peptides.</li>  
                        </ul>
                    </p>
                
                <h2>Modelling and Evaluation</h2>
                <h3>Phase 1</h3>
                <center><img src="drug/class.png" height="50%" width="50%"></center>
                <p>
                    In the first phase of our project, we developed a machine learning classification model to identify whether a given peptide is positive or negative for thrombin inhibitory activity. To improve the performance of the model, we employed several techniques such as:
                <ul>
                    <li><b>Normalization: </b>We used MinMaxScaler, a commonly used normalization technique, to scale our features to a specific range (in our case, between 0 and 1). This technique helped us to eliminate any potential bias that may arise from differences in the scales of our features.</li>
                    <li><b>Recursive Feature Elimination (RFE): </b>We used RFE, a feature selection technique, to select the most important features that contribute to the thrombin inhibitory activity of the peptides. RFE reduced the number of features from 572 to 165 as shown in the figure below, making the model more efficient and effective.</li>
                    <li><b>Hyperparameter Tuning: </b>We used RandomizedSearchCV, a technique for hyperparameter tuning, to find the optimal combination of hyperparameters for our machine learning classification model. Hyperparameters are adjustable parameters that determine the performance of the model. The tuning process helped us to identify the best combination of hyperparameters to achieve the highest accuracy for our model.</li>
                </ul>
                </p>
                <div class="clearfix" style="display:flex; align-items:center;">
                    <div class="img-container">
                        <img src="drug/Slide5.PNG" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="drug/classper.png" alt="Forest" style="width:100%">
                    </div>
                </div>
                <p>After comparing the performance of Support Vector Classification (SVC) with radial basis function (RBF) and SVC with linear kernel, we found that both performed better than other models. Therefore, we decided to build an ensemble model that consists of both SVC RBF and SVC Linear to further improve the accuracy and reliability of our classification model. The ensemble model combines the outputs of both models to make a final prediction, which often leads to better performance than a single model.</p>

                <h3>Phase 2</h3>
                <center><img src="drug/reg.png" height="50%" width="50%"></center>
                <p>
                    In phase 2 of the project, we aim to build a machine learning regression model to predict the inhibitory constant (Ki) of positive peptides, which measures the strength of the binding between the peptide and thrombin. 
                    Since there is much variation in the Ki values, we split the Ki values into two buckets, small and large. This was done to create a binary classification problem where we can predict if a peptide has a small or large Ki value. 
                    We built a classification model to predict which bucket a peptide belongs to using the features extracted from the positive peptide dataset.
                </p>
                <div class="clearfix" style="display:flex; align-items:center;">
                    <div class="img-container">
                        <img src="drug/sfs.png" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="drug/regper.png" alt="Forest" style="width:100%">
                    </div>
                </div>
                <p>We also built two separate regression models for each bucket to predict the Ki values of peptides within each bucket. The regression models were trained on the subset of positive peptides with either small or large Ki values, and used the same set of features selected through feature selection techniques such as SFS.                     
                    To improve the performance of the model, we employed Normalization, Hyperparameter tuning but instead of RFE we used Sequential Forward Selection method to reduce the number of features.
                
                <b>Sequential Forward Selection: </b>Feature selection technique used to select a subset of relevant features from a larger set of available features. In our project, we used SFS to identify the most relevant features for our regression model to predict the inhibitory constant (Ki) of positive peptides. After applying SFS, we identified a subset of 34 features that were most important for predicting Ki values. These 34 features were used in our regression model to achieve better performance and more accurate predictions.
                
                By splitting the Ki values into two buckets and building separate models for each, we were able to achieve better prediction accuracy for both classification and regression tasks.
                </p>

                <h4>Identification of new peptides with antithrombin activity</h4>
                <p align="justify">
                We collected 10 million peptides from protein database which we ran into the Two-staged ML pipeline as shown below:
                </p>
                <center><img src="drug/pipe.png" height="70%" width="70%"></center>
                <p>The collected peptides are from various sources as shown in the figure below. We selected the peptides by moving the decision function for the ensemble model. We did this to ensure the validity of the peptides and to be more confident about them. 
                Finally, our model identified 573 peptides that might potentially have antithrombotic properties. We then predicted Ki values for 573 peptides using classification and regression model built in the Phase 2. The Ki distributions of the predicted peptides are shown below:</p>
                <center><img src="drug/Slide7.PNG" height="70%" width="70%"></center>
                </p>
        
                <h4>Selecting Peptides for Wet-lab experiments</h4>
                <p  align="justify">
                Now that we have curated 573 peptides from 10 million test sets, we have to choose peptides for further analysis. Since, peptides with large sequence length is not easily digestible, we choose 18 peptides which has sequence length less than or equal to 25 for futher wet-lab experimental analysis.
                </p>
                <center><img src="drug/ki.png" height="50%" width="50%"></center>
                <p>
                We hope to find next potential drug to inhibit thrombin out of this 18.
                </p>
                <h2>Future Work</h2>
                <ul>
                    <li>Implementing NLP in the modelling phase, which will predict the peptides based on the sequences</li>
                    
                    <li>Implementing deep learning models.</li>
                </ul>
            </div>

            <!-- Project details here -->
            <!-- Aging Clock -->
            <div class="project-details" id="project_2">
                <h1><font size="10">Aging Clock</font>
                    <br>Prediction of Age using Biomarkers</br>
                </h1>
                <div class="container">    
                <center><img src="agingclock/gif.gif" height="25%" width="25%"></center>
                </div>
            <div>
                <h2>Problem Statement</h2>
                <p align="justify">
                    As humans, we all desire to live long and healthy lives. However, aging is a complex process that increases our risk of developing chronic diseases, impacting our overall health and well-being. To combat this, identifying biomarkers associated with healthy aging is crucial for developing interventions and treatments to delay or prevent age-related diseases, ultimately increasing our healthy lifespan. The problem lies in accurately and meaningfully identifying such biomarkers, which are key to assessing anti-aging therapies. Therefore, the challenge is to identify and validate reliable biomarkers for human aging, which can be used to develop effective treatments and improve the quality of life for older adults. 
                </p>
            </div>
        
                <h2>Goal</h2>
                <p align="justify">
                    This project aims to uncover biomarkers in blood and urine that are linked with human aging and to create an accurate regression model that can predict a person's age based on these biomarkers. By analyzing a comprehensive dataset of biomarkers from individuals of different ages and using advanced statistical and machine learning methods, this project seeks to identify key biomarkers that are associated with aging. Ultimately, this project aims to advance our knowledge of human aging and potentially lead to the development of interventions to support healthy aging and promote longevity.
                </p>
        
                <h2>Data Collection</h2>
                <p align="justify">
                    The dataset used in this project consists of biomarkers obtained from laboratory analysis of blood and urine samples. The biomarkers were collected from the National Health and Nutrition Examination Survey (<a href="https://wwwn.cdc.gov/nchs/nhanes/" style="color: rgb(27, 3, 3);">NHANES</a>), which is a program that gathers health and nutritional information from a representative sample of the US population. NHANES provides a valuable resource for studying the relationship between biomarkers and aging. The dataset used in this project consists of over 100,000 samples and 22 features including Age, Gender and other Biochemical profile.
                </p>    
                
                
                <h2>Exploratory Data Analysis</h2>
                <p alingn="justify">
                    Exploratory Data Analysis (EDA) is a crucial step in any data analysis project as it helps in understanding the characteristics and properties of the data. In this project, the data is analyzed through visualizations in using Matplotlib and Seaborn libraries.
                </p>
                    <h3>Analysis of Age</h3>
                    <center><img src="agingclock/pic/1 Age.png" width="700" height="250"></center>
                    <h4>Observation</h4>
                    <p>
                        <ul>
                            <li>Distribution of Age revealed that there are more than 10k samples of infants and toddlers (Age 0-2).</li>    
                            <li>25th percentile of the data covers the Age less than or equal to 10 whereas 50% of the data belongs to Age less than 24.</li>
                            <li>This suggests that the significant percentage of data belongs to Children and teenagers.</li>
                            <li>The distribution of Age between the Male and Female is similar, suggesting that the data is unbiased with respect to Gender.</li>
                        </ul>
                    </p>
        
                    <h3>Distributions of Biomarkers</h3>
                    <center><img src="agingclock/pic/3 Distribution.png" width="1100" height="900"></center>
                    
                    <h4>Observation</h4>
                    <p>The distribution of Biomarkers give us an insight about how the variable are distributed.
                        <ul>
                            <li>Potassium, Sodium, Calcium, Protein, RBC, Hemoglobin, Hematocrit and MCV are <b>Normally distributed.</b></li>
                            <li>Albumin, Glucose, Bilirubin, Alkaline Phosphate and Triglycerides are <b>heavily skewed on the right side.</b></li>  
                            <li>It is important to notice that the <b>most features are right skewed.</b></li>  
                        </ul>
                    </p>
                    
                    <br>
                    <h3>Correlations between the features</h3>
                    <center><img src="agingclock/pic/2 Correlation.png" width="800" height="500"></center>
                    
                    <h4>Observation</h4>
                    <p>From the correlation matrix we can see a strong correlation between:
                        <ul>
                            <li>Cholestrol and LDL</li>
                            <li>Hemoglobin and RBC</li>  
                            <li>Hematocrit and RBC</li>
                            <li>Hemoglobin and Hematocrit</li>
                        </ul>
                        Little or no correlation is found between other features.
                    </p>
        
                    <br>
                    <h3>Scatter plot and Median of Biomarkers over Age</h3>
                    
                    <div class="clearfix">
                        <div class="img-container">
                            <img src="agingclock/pic/7 Scatter.png" alt="Italy" style="width:100%">
                        </div>
                        <div class="img-container">
                        <img src="agingclock/pic/12 Median.png" alt="Forest" style="width:100%">
                        </div>
                    </div>
        
                    <h4>Observation</h4>
                    <p>The image in the left is the scatter plot between each features with respect to age, whereas the image in the right indicated the age-wise median values of features.
                        <ul>
                            <li>Although we couldn't infer much information from the scatter plot, the median plots gives us a clear variation of the Biomarkers over age.</li>  
                            <li>We can see biomarkers such as Albumin, Glucose, Urea and MCV increases over age.</li>
                            <li>Also, RBC and Platelets decreases over age.</li>
                            <li>This explains 45% correlation between Urea and MCV with respect to age.</li>
                        </ul>
        
                    <br>
                    <h3>Box plt with respect to Gender</h3>
                    
                    <center><img src="agingclock/pic/8 Gender Aft.png" width="1000" height="800"></center>
        
                    <h4>Observation</h4>
                    <p>
                        <ul>
                            <li>Prominent difference between the gender can be seein in RBC, Hemogloin and Hematocrit. This can be explained by the fact that the male has higher RBC count than female in general.</li>
                            <li>We can also see the differences in Urea, HDL, Bilirubin, Calcium and Creatinine as well.</li>
                        </ul>
                        <p><strong>Note:</strong> Albumin is log transformed as it is highly skewed, and the distribution with the box plot cannot be seen without transformation. Also, the outliers are removed for this figure to make the visualization clear and prominent.</p>
                    </p>
        
                <h2>Data Cleaning</h2>
                <p>
                    The dataset contains lot of missing value as shown in the figure below. Therefore, it is really important to clean the data before we build the regression model. We can see that Glucose, LDL and Triglycerides have around 70% of null values. It is really important to handle the null values carefully to avoid errors.
                    <br>
                    <center><img src="agingclock/pic/null.png"></center>
                    <h3>Handling Null Values and Outliers</h3>
                    <center><img src="agingclock/pic/Missing data.gif"></center>
        
                    Handling the null values are carried out in six steps as follow:
                    <h4>Step 1</h4>
                        <ul>
                            <li>During our exploratory data analysis, we observed that several rows in the dataset had missing values for all features except for age and gender.</li>
                            <li>Considering the potential increase in data imputation errors, we made the decision to remove these rows from our analysis.</li>
                        </ul>
                    <h4>Step 2</h4>
                    <ul>
                        <li>Upon reviewing the dataset, it has been observed that samples with an age less than or equal to 10 have missing values for features ranging from Glucose to Alkaline Phosphatase.</li>
                        <li>Thus, we had to removed samples with Age less than or equal to 10, confining the dataset from age 11 to 85.</li>
                    </ul>
                    <h4>Step 3</h4>
                    <center><img src="agingclock/pic/10 ldl vs chol.png"></center>
                    <ul>
                        <li>As indicated by the correlation matrix as well as the above figure, LDL exhibits a strong correlation with Cholesterol. However, the data for LDL is comparatively more sparse than that of Cholesterol.</li>
                        <li>Linear Regression is applied to impute LDL based on Cholesterol values.</li>
                    </ul>
                    <h4>Step 4</h4>
                    <ul>
                        <li>The remaining missing values are filled with the median of same age or the median bucketted age.</li>
                        <li>We choose Median instead of Mean because not all the features are normally distributed.</li>
                    </ul>
                    <h4>Step 5</h4>
                    <ul>
                        <li>The outliers are removed with respect to Inter Quartile Range (IQR)</li>
                        <li>Removing outliers helps to reduce the effect of extreme values that do not represent the normal distribution of the data.</li>
                    </ul>
                    
                
                </p>
                
                <h2>Modeling and Evaluation</h2>
        
                <div>
                <p align="justify">
                    In this project, various regression methods were applied to predict age based on the identified biomarkers. These methods include Linear Regression (LR), Ridge Regression (RR), Lasso Regression (LsR), Elastic Net Regression (ENR), Decision Tree Regression (DTR), Random Forest Regression (RFR), Gradient Boosting Regression (GBR), XGBoost Regression (XGBR), Support Vector Regression (SVR), and Multilayer Perceptron Regression (MLP).
                    In addition, to ensure the reliability of the models, 5-fold cross-validation was applied to evaluate the performance of each regression method. The mean squared error (MSE) and root mean squared error (RMSE) were calculated for each fold and the average values were used to assess the overall performance of the model. 
                </p>
                
                <h4>Techniques applied to improve the model</h4>
                <ul>
                    <li><b>Normalization: </b>The data is normalized using MinMaxScaler technique to ensure that they are all on the same scale.</li>
                    <li><b>PCA: </b>PCA is applied to reduce the number of features and eliminate any potential multicollinearity among them. The resulting principal components with 95% variance are then used as input variables in the regression models to predict age.</li>
                    <li><b>Hyperparameter Tuning: </b>GridSearchCV is used to find the best hyperparameters for all the model. The best parameters are then applied to optimize each model.</li>
                </ul>
                </div>
        
                <div class="clearfix" style="display:flex; align-items:center;">
                    <div class="img-container">
                        <img src="agingclock/pic/results.png" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="agingclock/pic/final.png" alt="Forest" style="width:100%">
                    </div>
                </div>
        
                <h4>Model Results</h4>
                <p align="justify">
                <ul>
                    Based on the results obtained from the regression models, we can make several observations.
                    <li>In the Baseline model, Random Forest, Gradient Boost and XGBoost Regressor models have demonstrated superior performance compared to the other models.</li>
                    <li>However, applying normalization to the data has not shown any improvement in the performance of the models. Surprisingly, it has even increased the error for Lasso Regressor and ElasticNet Regressor.</li>
                    <li>When applied PCA to the data, the model's performance has declined drastically for all the models. Therefore, we can conclude that applying PCA may not be suitable for this dataset.</li>
                    <li>The hyperparameter tuning has led to better performance in the models.</li>
                    <li>Upon closer inspection of the right figure, it can be observed that there is no significance difference between the training and validation scores for all models, except the top-performing Random Forest, Gradient Boost, and XGBoost Regressor.</li>
                    <li>By comparing the model performance, we choose <b>Random Forest Regressor</b> as the best model since it has less RMSE and less difference between the training and validation scores among the best performing models.</li>
                </ul>
                </p>
        
                <h4>Feature Importance</h4>
                <p  align="justify">
                Identifying the key biomarkers involved in aging is one the goal of this project. Since, correlation matrix only captures the linear relationship, <b>Permutation Feature Importance</b> method is used with the best performing <b>Random Forest Regressor</b> and <b>Gradient Boosting Regressor</b> models to identify features that has significant influence on the aging. This method recognizes non-linear relationship between the features and the target that could not be captures by the correlation matrix.
                </p>
                
                <div class="clearfix" style="display:flex; align-items:center;">
                    <div class="img-container">
                        <img src="agingclock/pic/ft rfr.png" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="agingclock/pic/ft gbr.png" alt="Forest" style="width:100%">
                    </div>
                </div>
        
                <ul>
                    Based on the Permutation Feature Importance method, we can make several observations.
                    <li>Glucose is the strong indicator of age followed by Triglycerides.</li>
                    <li>Alkaline Phosphatase and Urea is the third and fourth important features. This is in line with the correlation matrix, as these two features had 45% correlation with Age.</li>
                    <li>Bilirubin and blood indicators such as Hemoglobin, Hematocrit are poor indicators of age according to this findings.</li>
                </ul>
               
        
                <!-- <h2>Conclusions and Drawbacks</h2> -->
                
                
                <h2>Future Work</h2>
                <ul>
                    <li>Rerun the model by removing the rows with any null values to see whether the data imputation has any effect on the Modeling performance or Permutation Feature Importance method.</li>
                    <li>Include demographic and other examination data from NHAHES website.</li>
                    <li>Implementing deep learning models.</li>
                </ul>
            </div>

            <!-- Project details here -->
            <!-- Swipe Right -->
            <div class="project-details" id="project_3">
                <h1><font size="10">swipe right</font>
                    <br>A Comparative Analysis of Popular Dating Apps</br>
                </h1>
                
                <div class="container">
                    
                <center><img src="dating/swipe.gif" height="25%" width="25%"></center>
                </div>
                
                <div>
                <h2>Problem Statement</h2>
                <p align="justify">
                    The COVID-19 pandemic has brought significant changes to the way we live our lives, including the way we connect and form relationships. Dating apps have emerged as a critical tool for people to maintain their social lives while adhering to social distancing measures. However, as dating apps have become more widely used, so too have concerns about the quality of the user experience. To address these concerns, analyzing Google app reviews of dating apps can be a valuable tool for identifying common issues and trends. By doing so, app developers can gain a deeper understanding of user needs and preferences and make targeted improvements to enhance the overall user experience. This can lead to higher levels of satisfaction, increased success rates, and improved safety measures for users of dating apps during the pandemic and beyond. In short, analyzing Google app reviews is a crucial step in ensuring that dating apps continue to meet the evolving needs of their users, and remain a reliable and valuable tool for fostering meaningful connections.
                </p>
                </div>
        
                <h2>Goal</h2>
                <p align="justify">
                    The aim of this project is to analyze the performance of four dating apps - Bumble, Hinge, Match, and Tinder - during the COVID-19 pandemic by conducting a sentiment analysis of their Google app reviews. By analyzing the user feedback, we can gain valuable insights into the common issues and trends experienced by users and identify their needs and preferences. Moreover, we will visualize the app's ratings over time, before, during, and after the pandemic, to determine any significant changes in user behavior and preferences. While we have used word cloud and Vader sentimental analysis to evaluate each app's sentiment, we can draw meaningful conclusions from our analysis and provide app developers with the necessary information to improve their app's user experience, safety measures, and success rates. This project serves as an important tool for app developers to ensure that their apps continue to meet the evolving needs of their users and foster meaningful connections.
                </p>
        
                <h2>Data Collection</h2>
                <p align="justify">
                    The dataset used in this project consists of biomarkers obtained from laboratory analysis of blood and urine samples. The biomarkers were collected from the National Health and Nutrition Examination Survey (<a href="https://wwwn.cdc.gov/nchs/nhanes/" style="color: rgb(27, 3, 3);">NHANES</a>), which is a program that gathers health and nutritional information from a representative sample of the US population. NHANES provides a valuable resource for studying the relationship between biomarkers and aging. The dataset used in this project consists of over 100,000 samples and 22 features including Age, Gender and other Biochemical profile.
                </p>    
                
                
                <h2>Preprocessing</h2>
                <p alingn="justify">
                    In this project, we used various natural language processing (NLP) techniques to analyze the user reviews of the dating apps. These includes:
                    <ul>
                        <li><b>Tokenization:</b> We used tokenization to break down the user reviews into individual words or tokens, making it easier to analyze the text.</li>
                        <li><b>Stop word removal:</b> Stop words are common words such as "the," "and," and "is" that do not provide any meaningful information for analysis. We removed these stop words to improve the accuracy of our analysis.</li>
                        <li><b>Stemming:</b> We used stemming to reduce words to their root form, so that variations of the same word could be treated as a single term. This allowed us to identify patterns in the text more easily.</li>
                    </ul>
                </p>

                <h2>Techniques Used</h2>
                <p alingn="justify">
                    We adopted several techniques including:
                    <ul>
                        <li><b>Vader Sentiment Analysis:</b> We used the VADER (Valence Aware Dictionary and sEntiment Reasoner) system, a pre-trained sentiment analysis tool, to analyze the sentiment of the user reviews. It assigns a sentiment score ranging from -1 (negative) to 1 (positive) to each review, allowing us to determine the overall sentiment expressed by users.</li>
                        <li><b>Word Clouds:</b> We used word clouds to visually represent the most frequently occurring words in the user reviews for each of the dating apps. This helped us to identify the most common themes and sentiments expressed by users and gain insights into their experiences with the app.</li>
                        <li><b>Visualization using Plotly:</b> We used Plotly, a data visualization library, to create interactive and dynamic visualizations of the user ratings over time. This allowed us to compare the app's performance before, during, and after the pandemic and identify any significant changes in user behavior and preferences over time. The visualizations provided a more intuitive and insightful way to analyze the data and gain insights into user behavior.</li>
                    </ul>
                </p>

                <h2>Findings</h2>
                <h4>Word Clouds</h4>
                <p>Now! Lets see the common theme and sentiments of the users.</p>
                <div class="clearfix">
                    <div class="img-container">
                        <img src="dating/bumble.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="dating/hinge.png" alt="Forest" style="width:100%">
                    </div>
                </div>
                <div class="clearfix">
                    <div class="img-container">
                        <img src="dating/match.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="dating/tinder.png" alt="Forest" style="width:100%">
                    </div>
                </div>
                
                <h4>Overall Analysis</h4>
                <p>How users feel about the app?</p>

                <center><img src="dating/overall.png" height="90%" width="90%"></center>

                <h4>Peformance during Pandemic</h4>
                <p>Which app performed better when it is needed the most?</p>

                <center><img src="dating/pandemic.png" height="90%" width="90%"></center>

                <h4>Rating over time</h4>
                <p>Sentimental Variations!!!</p>

                <center><img src="dating/ratingovertime.png" height="90%" width="90%"></center>
                    
                <br>
                <h3>Key Insights</h3>
                <p>Here are some insights we could get from the above analysis</p>
                <ul>
                    <li>Fake profiles are a major concern for users of dating apps, as they undermine trust and authenticity. Dating app developers need to implement measures to prevent and detect fake profiles to prioritize user safety and satisfaction.</li>
                    <li>Users of Bumble and Tinder have expressed frustration with the apps, with some feeling that they are a waste of time. Additionally, some users have reported being banned from Tinder, which has further added to their frustration and negative sentiment towards the app.</li>
                    <li>Although negative feedback can be found for dating apps such as Bumble, Hinge, Match, and Tinder, it's important to note that there are also positive comments from many satisfied users who appreciate the app's features and performance.</li>
                    <li>On analyzing the overall ratings, Hinge has performed better.</li>
                    <li>The COVID-19 pandemic had a significant impact on the user ratings of all the dating apps. However, Match was an exception to this trend and did not experience a significant decrease in user ratings during the pandemic.</li>
                    <li>Over the period from 2018 to 2022, there was a noticeable shift in user sentiment towards dating apps, as reflected in the user ratings. This shift can provide valuable insights into changes in user behavior and preferences over time, which can be useful for app developers to improve the user experience and satisfaction.</li>
                </ul>
                
                
                <h2>Future Work</h2>
                <ul>
                    <li>Analyze the sentiments with word clouds before and after pandemic to see if there any differences between both.</li>
                    <li>Include more data other than Google reviews to get more unbiased insights on the user experience.</li>
                    <li>Include few more top performing apps.</li>
                </ul>
            </div>
            
            <!-- preowned cars -->
            <div class="project-details" id="project_4">
                <h1><font size="10">Driven by the data</font>
                    <br>Big Data Analytics of the Preowned car market</br>
                </h1>
                <div class="container">
                    
                    <center><img src="car/gip.gif" height="25%" width="25%"></center>
                    </div>
                <h2>Goal</h2>
                <p align="justify">
                    The objective of this project is to leverage AWS to build a data pipeline for a pre-owned car dataset to analyze the factors that impact the price of the car, such as mileage, year of manufacture, make and model, and location. The dataset comprises information on the price, mileage, year of manufacture, make and model, and location of pre-owned cars. The project aims to gain insights into the factors that affect the car's price and use visualizations to interpret and communicate the results effectively. With the data pipeline, the project aims to automate data processing and analysis tasks to increase efficiency and scalability.
                </p>
        
                <h2>Data Collection</h2>
                <p align="justify">
                    In the data collection process for this project, we obtained the pre-owned car dataset with 3 million samples from Kaggle datasets, which contains information on the price, mileage, year of manufacture, make and model, and location of over 3 million pre-owned cars in the United States. Additionally, we retrieved data from Craigslist, a popular online marketplace for buying and selling used cars. To ensure the dataset's geographic accuracy, we extracted US zipcodes from Simplemaps and mapped them to the car's location data. 
                </p>
                
                <h2>Data Preprocessing</h2>
                <p align="justify">
                    In the data preprocessing phase using AWS Glue, we performed the following steps on the pre-owned car dataset:
                </p>
                <ul>
                    <li>Removing rows with inadequate data: We removed any rows that had missing or incomplete data, which could skew our analysis and lead to inaccurate insights.</li>
                    <li>Removing duplicates: We eliminated any duplicate rows to ensure that our analysis is based on unique data points.</li>
                    <li>Removing irrelevant columns: We dropped any columns that were not essential to our analysis, such as unique identifiers or redundant information.</li>
                    <li>Data transformations: We performed data transformations to convert data types and standardize the format of the data, making it easier to analyze.</li>
                    <li>Imputing null values: We imputed null values using different techniques such as mean, median or mode, depending on the column's nature, to avoid losing too much data.</li>
                </ul>
                <p>Overall, these data preprocessing steps using AWS Glue helped us to clean and standardize the data, making it more suitable for analysis and machine learning.</p>
                
                <h2>Data pipeline</h2>
                <p>In the data pipeline for this project, we used AWS services such as S3, Redshift/RDS, and visualization tools to store, process, and analyze the pre-owned car dataset.</p>
                <p>First, we stored the pre-processed data in an S3 bucket, which allows us to easily and securely store and retrieve large amounts of data. Next, we used both Redshift and RDS, depending on the nature of the analysis, to load and transform the data into a structured format that is optimized for querying and analysis. Finally, we used visualization tools such as Tableau to create interactive visualizations that enable us to communicate insights effectively.
                <p>Overall, this data pipeline allowed us to store, process, and analyze large amounts of data efficiently, making it easier to extract insights and create visualizations that communicate those insights effectively.</p>             
                <div class="clearfix">
                    <div class="img-container">
                        <img src="car/s3.png" alt="Italy" style="width:100%" valign="center">
                    </div>
                    <div class="img-container">
                    <img src="car/flow.png" alt="Forest" style="width:100%" valign="center">
                    </div>
                </div>
    
                <div class="clearfix">
                    <div class="img-container">
                        <img src="car/rds.png" alt="Italy" style="width:100%" valign="center">
                    </div>
                    <div class="img-container">
                    <img src="car/redshift.png" alt="Forest" style="width:100%" valign="center">
                    </div>
                </div>
                
                <h2>Data Visualization</h2>
                <div class="clearfix">
                    <div class="img-container">
                        <img src="car/1.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="car/2.png" alt="Forest" style="width:100%">
                    </div>
                </div>

                <h2>Key Inferences</h2>
                <p>Based on our analysis of the pre-owned car dataset, we found that the following factors have a significant impact on the price of the car:</p>
                <ul>
                    <li><b>Mileage:</b> The number of miles covered by the car is a key determinant of its price. Cars with lower mileage are priced higher than those with higher mileage.</li>
                    <li><b>Accidents:</b> The presence of any accidents or damage history significantly reduced the car's price. Cars with a clean accident history are generally priced higher.</li>
                    <li><b>Make and model:</b> Certain car makes and models such as Pagani, Bugatti are associated with higher prices due to their perceived reliability, prestige, or popularity.</li>
                    <li><b>Listing color:</b> The color of the car also impacts the price, with popular colors such as black, white, and yellow generally priced higher than less popular colors.</li>
                    <li><b>City:</b> The location of the car can also influence its price.</li>
                    <li><b>Fuel type: </b>The type of fuel the car uses also affect the price, with hybrid and electric cars generally priced higher than gasoline-powered cars.</li>
                </ul>
                
                
                <!-- <h2>Future Work</h2>
                <ul>
                    <li>Streamlining the data to upda.</li>
                </ul> -->
            </div>

            <!-- covid -->
            <div class="project-details" id="project_5">
                <h1><font size="10">Mapping the Pandemic</font>
                    <br>A Geographic Analysis of COVID Data in US</br>
                </h1>
        
                <h2>Goal</h2>
                <p align="justify">
                    The goal of the project is to analyze the COVID-19 data for cases and deaths in the United States to gain insights into the spread and impact of the virus and inform public health policies and interventions. By analyzing the data, we aim to identify patterns, trends, and factors contributing to changes in cases and deaths over time, demographic groups affected, regional differences, and effective public health policies and interventions. The ultimate goal is to use this information to help mitigate the impact of the pandemic on public health, the economy, and society.
                </p>
        
                <h2>Data Collection</h2>
                <p align="justify">
                    We obtained the COVID-19 data for cases and deaths in the United States from two datasets available on Kaggle. The first dataset contains information on COVID-19 cases and deaths at the state level and has 58,000 rows. The second dataset contains information on COVID-19 cases and deaths at the county level and has 2.5 million rows.
                </p>    

                <h2>Visualizations</h2>
                
                <div class="clearfix">
                    <div class="img-container">
                        <img src="covid/total cases.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="covid/total death.png" alt="Forest" style="width:100%">
                    </div>
                </div>

                <div class="clearfix">
                    <div class="img-container">
                        <img src="covid/average.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="covid/counties.png" alt="Forest" style="width:100%">
                    </div>
                </div>
                
                <h2>Key Inferences</h2>
                <p>Here are some insights we could get from the above analysis</p>
                <ul>
                    <li>All the years combined California has highest Covid-19 cases and deaths over other states. This is followed by Texas and Florida.</li>
                    <li>American Samao has the least number of cases and deaths.</li>
                    <li>Rhode Island has the highest average cases per 100k where Arizona has highest average deaths per 100k.</li>
                    <li>Maine and Northern Mariana Islands have lowest average cases and deaths per 100k respectively.</li>
                </ul>
                
                
                <h2>Future Work</h2>
                <ul>
                    <li>Combining other datasets and analyze the impact during COVID in each states and county.</li>
                </ul>
            </div>

            <!-- NBA Basketball Analytics -->
            <div class="project-details" id="project_6">
                <h1><font size="10">Dribbling Data</font>
                    <br>A Database Management Project Analyzing NBA</br>
                </h1>
                
                <div class="container">
                    
                <center><img src="nba/dunk.gif" height="25%" width="25%"></center>
                </div>
                
                <div>
                <h2>Problem Statement</h2>
                <p align="justify">
                    In today's data-driven world, the use of database management systems has become increasingly important for businesses and organizations to store and manage their data effectively. The National Basketball Association (NBA) is no exception, as it generates vast amounts of data each season, including player statistics, team rankings, and game scores. To make informed decisions, it is important to build high-performing database management systems that can handle large amounts of data and support efficient querying for analysis. This project aims to build and compare the performance of two popular database management systems, MySQL and MongoDB, using JMeter with NBA data. By analyzing the data and gaining insights into trends and patterns, the project can help organizations and NBA enthusiasts make informed decisions to improve player and team performance, inform strategic decision-making.
                </p>
                </div>
        
                <h2>Goal</h2>
                <p align="justify">
                    This project aims to build two database management systems, MySQL and MongoDB, and compare their performance using JMeter with NBA data. The project also involves analyzing NBA stats to gain useful insights from the data. The first part involves building the databases and populating them with NBA data. The second part involves testing the performance of the databases using JMeter. The final part involves analyzing the NBA stats to gain insights into player performance, team performance, and trends in the NBA. The project aims to compare the performance of MySQL and MongoDB databases and gain insights into the NBA data using database management systems and performance testing tools.
                </p>
        
                <h2>Data Collection</h2>
                <p align="justify">
                    The data for this project was collected from the NBA website through web scraping, resulting in more than 60,000 games, 30 teams, 4.5 players, and their statistics. The collected data was then distributed among 16 tables, each containing specific information related to the NBA, including game scores, team rankings, player statistics, and more. The data spans from the year 1946 to 2020, covering over seven decades of NBA history. The use of web scraping techniques enabled the collection of a large amount of data, providing a comprehensive dataset for analysis and insights into trends and patterns over time.
                </p>    
                
                <h2>Workflow</h2>        
                <center><img src="nba/workflow.png"></center>
                <p>The figure above illustrates the flow of data in the database systems. The data is processing in the database and imported  into a cloud-based database management system. This will provide several benefits, such as improved scalability, availability, and security. Once the data is in the cloud, it is then connected to the visualization tools for further analysis.</p>
                <h4>Performance Comparison between MySQL and MongoDB</h4>
                <div class="clearfix">
                    <div class="img-container">
                        <img src="nba/mysql.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="nba/mongodb.png" alt="Forest" style="width:100%">
                    </div>
                </div>
                <p>From the above graph, we can see MongoDB performs better compared to MySQL.</p>

                <h2>Analysis of NBA Stat</h2>
                
                <div class="clearfix">
                    <div class="img-container">
                        <img src="nba/game.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="nba/homeaway.png" alt="Forest" style="width:100%">
                    </div>
                </div>

                <div class="clearfix">
                    <div class="img-container">
                        <img src="nba/offense.png" alt="Italy" style="width:100%">
                    </div>
                    <div class="img-container">
                    <img src="nba/defense.png" alt="Forest" style="width:100%">
                    </div>
                </div>
                
                <center><img src="nba/player.png" height="70%" width="70%"></center>

                <h2>Key Inferences</h2>
                <p>Here are some insights we could get from the above analysis</p>
                <ul>
                    <li>The chance of winning as a home team is higher than as an opposite team.</li>
                    <li>Moderate to higher correlation between the winning rate of the team and the 3 points score efficiency, free throw percentage, rebound percentage.</li>
                    <li>Although negative feedback can be found for dating apps such as Bumble, Hinge, Match, and Tinder, it's important to note that there are also positive comments from many satisfied users who appreciate the app's features and performance.</li>
                    <li>The team with both good offensive and defensive strategies have a high winning percentage.</li>
                    <li>No significant correlation between the Team salary with their performance and Player Salary with their performance.</li>
                </ul>
                
                
                <h2>Future Work</h2>
                <ul>
                    <li>Streamlining the data.</li>
                </ul>
            </div>
            
            <div class="project-details" id="project_7"></div>
            <div class="project-details" id="project_8"></div>
            <div class="project-details" id="project_9"></div>
            <div class="project-details" id="project_10"></div>
        </div>
        <div class="overlay-footer"></div>
     </div>

    <footer>
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <span class="copyright">Copyright 2023 &copy; Nivedha Balakrishnan</span>
                </div>
                <div class="col-md-4">
                    <ul class="list-inline social-buttons">
                        <li><a href="https://www.linkedin.com/in/nivedhabkr" target="_blank"><i class="fa fa-linkedin"></i></a>
                        </li>
						<li><a href="https://github.com/NivedhaBalakrishnan" target="_blank"><i class="fa fa-github"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery/jquery.min.js"></script>
	
	<!--LinkedIn Badge-->
	<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>
	
    <script src="js/bootstrap/js/bootstrap.min.js"></script>
	<script src="js/jquery/jquery.easing.min.js"></script>
    <script src="js/scripts.js"></script>

</body>

</html>
